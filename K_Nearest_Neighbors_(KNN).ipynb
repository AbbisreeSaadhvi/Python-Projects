{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuoCvIqSY8rO1zkv6K3Oa2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbbisreeSaadhvi/Python-Projects/blob/main/K_Nearest_Neighbors_(KNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbors (KNN)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm that is used for classification and regression tasks. In KNN, the target value of a data point is determined by the majority class (for classification) or the average (for regression) of its k-nearest neighbors in the feature space.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "1. Choosing the number of neighbors (k).\n",
        "2. Calculating the distance between the data points.\n",
        "3. Sorting the data points based on distance.\n",
        "4. Voting for the majority class or averaging the values.\n",
        "\n",
        "**Dataset:** Titanic Dataset\n"
      ],
      "metadata": {
        "id": "P6FLWcMmEQ3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Titanic dataset from GitHub\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Drop columns that are not useful for prediction\n",
        "data = data.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(columns=['Survived'])\n",
        "y = data['Survived']\n",
        "\n",
        "# Preprocess the data\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Sex', 'Embarked']\n",
        "numerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "# Create transformers for preprocessing\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Create a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Create a pipeline that preprocesses the data and then fits the KNN model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', knn)\n",
        "])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk0-Gd1GEd9L",
        "outputId": "f0ebd1c4-3831-492b-902e-4ab46a4689f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "Accuracy: 0.8044692737430168\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       105\n",
            "           1       0.79      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**\n",
        "*Accuracy:* The model has an accuracy of approximately 80.44%, meaning it correctly predicts whether a passenger survived or not about 80.44% of the time.\n",
        "\n",
        "**Classification Report:**\n",
        "\n",
        "*Precision:* The precision for class 0 (not survived) is 0.81, meaning that 81% of the passengers predicted not to survive did not survive. For class 1 (survived), the precision is 0.79, meaning 79% of the passengers predicted to survive actually survived.\n",
        "\n",
        "\n",
        "*Recall:* The recall for class 0 is 0.87, meaning that 87% of the actual not-survived passengers were correctly identified. For class 1, the recall is 0.72, meaning 72% of the actual survived passengers were correctly identified.\n",
        "\n",
        "\n",
        "*F1-Score:* The F1-score is the harmonic mean of precision and recall. For class 0, it is 0.84, and for class 1, it is 0.75.\n",
        "\n",
        "\n",
        "*Support:* The support shows the number of actual occurrences of each class in the test set (105 for class 0 and 74 for class 1).\n"
      ],
      "metadata": {
        "id": "rJOkZenSFUoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "\n",
        "The KNN model performed reasonably well on the Titanic dataset, with an overall accuracy of approximately 80.44%. The model is slightly better at predicting passengers who did not survive (class 0) compared to those who survived (class 1). This is evident from the higher precision, recall, and F1-score for class 0. Further improvements could involve tuning the hyperparameters, exploring more sophisticated preprocessing steps, or trying different machine learning algorithms."
      ],
      "metadata": {
        "id": "hU86zr1ZGGMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**\n",
        "\n",
        "*SimpleImputer:*\n",
        "\n",
        "This transformer is used to handle missing values. For numerical columns, we use the strategy mean to replace missing values with the mean of the column. For categorical columns, we use the strategy most_frequent to replace missing values with the most frequent value in the column.\n",
        "\n",
        "*Pipeline for Preprocessing:*\n",
        "\n",
        "1. numerical_transformer: This pipeline handles numerical columns by first imputing missing values with the mean and then scaling the values.\n",
        "2. categorical_transformer: This pipeline handles categorical columns by first imputing missing values with the most frequent value and then applying one-hot encoding.\n",
        "3. ColumnTransformer: This transformer combines the numerical and categorical transformers, applying them to the respective columns.\n",
        "\n",
        "By adding these steps, the missing values in the dataset are handled."
      ],
      "metadata": {
        "id": "DtRLKvdtGu0R"
      }
    }
  ]
}